{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV4V6pnYBaMm",
        "outputId": "42b281f5-41fe-4862-9af3-cafb0b1834d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install  -q groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from groq import Groq\n",
        "from typing import List, Dict, Union, Optional"
      ],
      "metadata": {
        "id": "BGqbP599BhxV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = \"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\""
      ],
      "metadata": {
        "id": "9XbdvG8WBrkS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'llama-3.1-8b-instant'\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "print(\"Groq client initialized successfully.\")"
      ],
      "metadata": {
        "id": "ysqM0QQzBw3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34406e7-38a6-4f5d-bb34-903d0679f282"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq client initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationManager:\n",
        "    \"\"\"\n",
        "    Manages a conversation history with summarization and truncation\n",
        "    using only the Groq API and standard Python.\n",
        "    \"\"\"\n",
        "    def __init__(self, client: Groq, model: str, summarize_every_k: int = 3):\n",
        "        \"\"\"\n",
        "        Initializes the manager.\n",
        "\n",
        "        Args:\n",
        "            client: The initialized Groq client.\n",
        "            model: The model name to use (e.g., 'llama3-8b-8192').\n",
        "            summarize_every_k: How often to summarize (e.g., every 3 runs).\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.summarize_every_k = summarize_every_k\n",
        "        self.run_count = 0\n",
        "        # Start with a base system prompt\n",
        "        self.history: List[Dict[str, str]] = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "        ]\n",
        "\n",
        "    def add_message(self, role: str, content: str):\n",
        "        \"\"\"Adds a message to the *full* conversation history.\"\"\"\n",
        "        self.history.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    def _summarize(self, text_to_summarize: str) -> str:\n",
        "        \"\"\"\n",
        "        Internal method to call the Groq API for summarization.\n",
        "        \"\"\"\n",
        "        if not text_to_summarize:\n",
        "            return \"Nothing to summarize.\"\n",
        "\n",
        "        system_prompt = \"Summarize the following conversation concisely. Capture the key topics, any questions asked, and the main answers given. This summary will be used as context for a future conversation.\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": text_to_summarize}\n",
        "                ],\n",
        "                temperature=0.1\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error during summarization: {e}\")\n",
        "            return \"[Summarization failed]\"\n",
        "\n",
        "    def _perform_periodic_summarization(self):\n",
        "        \"\"\"\n",
        "        Checks if it's time to summarize and, if so, replaces the\n",
        "        conversation history with a summary.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- [Attempting Summarization at Run {self.run_count}] ---\")\n",
        "\n",
        "        # We summarize *after* the k-th run is complete\n",
        "        if self.run_count > 0 and self.run_count % self.summarize_every_k == 0:\n",
        "            print(f\"Condition met (k={self.summarize_every_k}). Summarizing history...\")\n",
        "\n",
        "            # Keep system prompts, but summarize the rest\n",
        "            system_prompts = [msg for msg in self.history if msg['role'] == 'system']\n",
        "            conversation_part = [msg for msg in self.history if msg['role'] != 'system']\n",
        "\n",
        "            if not conversation_part:\n",
        "                print(\"No conversation to summarize. Skipping.\")\n",
        "                print(\"-------------------------------------------------\")\n",
        "                return\n",
        "\n",
        "            # Format conversation for the summarizer model\n",
        "            text_to_summarize = \"\\n\".join(\n",
        "                [f\"{m['role']}: {m['content']}\" for m in conversation_part]\n",
        "            )\n",
        "\n",
        "            print(f\"Original History Length (non-system): {len(conversation_part)} messages\")\n",
        "\n",
        "            # Get the summary from the API\n",
        "            summary = self._summarize(text_to_summarize)\n",
        "\n",
        "            print(f\"New Summary: {summary}\")\n",
        "\n",
        "            # The new history is just the system prompts + the new summary\n",
        "            self.history = system_prompts + [\n",
        "                {\"role\": \"system\", \"content\": f\"Summary of the conversation so far: {summary}\"}\n",
        "            ]\n",
        "            print(f\"History has been replaced with summary. New history length: {len(self.history)}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Condition not met (Run {self.run_count} % {self.summarize_every_k} != 0). Skipping summarization.\")\n",
        "\n",
        "        print(\"-------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def get_truncated_history(self, max_turns: Optional[int] = None, max_chars: Optional[int] = None) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Returns a *copy* of the history, truncated according to the rules.\n",
        "        It does NOT modify the main `self.history`.\n",
        "        This is used to prepare the message list for the *next* API call.\n",
        "        \"\"\"\n",
        "\n",
        "        # Always keep system prompts\n",
        "        system_prompts = [msg for msg in self.history if msg['role'] == 'system']\n",
        "        conversation_part = [msg for msg in self.history if msg['role'] != 'system']\n",
        "\n",
        "        # 1. Truncate by number of turns (1 turn = 1 user + 1 assistant)\n",
        "        if max_turns:\n",
        "            num_messages_to_keep = max_turns * 2\n",
        "            # Get the last N messages\n",
        "            start_index = max(0, len(conversation_part) - num_messages_to_keep)\n",
        "            conversation_part = conversation_part[start_index:]\n",
        "\n",
        "        # 2. Truncate by character length\n",
        "        elif max_chars:\n",
        "            current_chars = 0\n",
        "            temp_list = []\n",
        "            # Iterate in reverse to keep the *latest* messages that fit\n",
        "            for message in reversed(conversation_part):\n",
        "                msg_len = len(message['content'])\n",
        "                if current_chars + msg_len > max_chars:\n",
        "                    break\n",
        "                current_chars += msg_len\n",
        "                temp_list.append(message)\n",
        "            conversation_part = list(reversed(temp_list)) # Re-reverse to correct order\n",
        "\n",
        "        # Return the system prompts + the (potentially truncated) conversation\n",
        "        return system_prompts + conversation_part\n",
        "\n",
        "    def chat(self, user_message: str, max_turns: Optional[int] = None, max_chars: Optional[int] = None) -> str:\n",
        "        \"\"\"\n",
        "        This is the main function to interact with the chat.\n",
        "        \"\"\"\n",
        "        if self.client is None:\n",
        "            return \"ERROR: Groq client is not initialized. Please check your API key.\"\n",
        "\n",
        "        # 1. Add the new user message to the full history\n",
        "        self.add_message(\"user\", user_message)\n",
        "\n",
        "        # 2. Get the history to send to the API (which might be truncated)\n",
        "        messages_to_send = self.get_truncated_history(max_turns=max_turns, max_chars=max_chars)\n",
        "\n",
        "        # 3. Call Groq API\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=messages_to_send\n",
        "            )\n",
        "            assistant_response = response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error during chat completion: {e}\")\n",
        "            assistant_response = \"[API Error]\"\n",
        "\n",
        "        # 4. Add the assistant's response to the full history\n",
        "        self.add_message(\"assistant\", assistant_response)\n",
        "\n",
        "        # 5. Increment the run count\n",
        "        self.run_count += 1\n",
        "\n",
        "        # 6. Check if it's time to summarize (this happens *after* the run)\n",
        "        self._perform_periodic_summarization()\n",
        "\n",
        "        return assistant_response"
      ],
      "metadata": {
        "id": "fq01U1u3B6Sq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if client:\n",
        "    # Initialize the manager to summarize after every 3rd run\n",
        "    manager = ConversationManager(client, MODEL, summarize_every_k=3)\n",
        "\n",
        "    print(f\"### RUN 1 (k=3) ###\")\n",
        "    response_1 = manager.chat(\"Hi there! My name is Alex. Can you tell me about the planet Mars?\")\n",
        "    print(f\"\\nUSER: Hi there! My name is Alex. Can you tell me about the planet Mars?\")\n",
        "    print(f\"ASSISTANT: {response_1}\")\n",
        "    print(f\"\\nFull history length: {len(manager.history)}\") # System + User + Assistant = 3\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(f\"### RUN 2 (k=3) ###\")\n",
        "    response_2 = manager.chat(\"How long would it take to travel there from Earth?\")\n",
        "    print(f\"\\nUSER: How long would it take to travel there from Earth?\")\n",
        "    print(f\"ASSISTANT: {response_2}\")\n",
        "    print(f\"\\nFull history length: {len(manager.history)}\") # 3 + User + Assistant = 5\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(f\"### RUN 3 (k=3) ###\")\n",
        "    # This is the 3rd run, so summarization will trigger *after* this\n",
        "    response_3 = manager.chat(\"What about Jupiter's 'Great Red Spot'?\")\n",
        "    print(f\"\\nUSER: What about Jupiter's 'Great Red Spot'?\")\n",
        "    print(f\"ASSISTANT: {response_3}\")\n",
        "    print(f\"\\nFull history length after summarization: {len(manager.history)}\") # Should be 2 (System + New Summary)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    print(f\"### RUN 4 (k=3) ###\")\n",
        "    print(\"Notice: The model will now use the summary as context.\")\n",
        "    # The model should know \"Alex\" and \"Mars\" from the summary\n",
        "    response_4 = manager.chat(\"Remind me, what was the first planet I asked about?\")\n",
        "    print(f\"\\nUSER: Remind me, what was the first planet I asked about?\")\n",
        "    print(f\"ASSISTANT: {response_4}\")\n",
        "    print(f\"\\nFull history length: {len(manager.history)}\") # 2 + User + Assistant = 4\n",
        "else:\n",
        "    print(\"Client not initialized. Skipping Task 1 Demonstration.\")"
      ],
      "metadata": {
        "id": "CXjqbTwVCHIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f50be9f6-e1b3-4a5f-aafa-f4cedeff534f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### RUN 1 (k=3) ###\n",
            "\n",
            "--- [Attempting Summarization at Run 1] ---\n",
            "Condition not met (Run 1 % 3 != 0). Skipping summarization.\n",
            "-------------------------------------------------\n",
            "\n",
            "USER: Hi there! My name is Alex. Can you tell me about the planet Mars?\n",
            "ASSISTANT: Hello Alex.  It's great to meet you. Mars, also known as the Red Planet, is the second-smallest planet in our solar system and the fourth planet from the Sun. It's a significant target for exploration due to its proximity to Earth and its potential for supporting past or present life.\n",
            "\n",
            "Here are some interesting facts about Mars:\n",
            "\n",
            "1. **Geological History**: Mars is believed to have formed around the same time as Earth, about 4.5 billion years ago. Scientists think that Mars may have had a more Earth-like environment in the past, with flowing rivers, lakes, and even oceans. However, the planet underwent a catastrophic event, possibly due to a massive asteroid impact, which caused widespread destruction and loss of its atmosphere.\n",
            "\n",
            "2. **Atmosphere**: The Martian atmosphere is very thin compared to Earth's. It's mostly composed of carbon dioxide, with some nitrogen and argon. The atmosphere is too thin to support liquid water on the surface, which makes it difficult for life to thrive.\n",
            "\n",
            "3. **Seasons**: Mars has a tilted axis, which results in seasonal changes similar to those on Earth. However, its orbital period is longer, taking about 687 Earth days to complete one orbit. The Martian year has four seasons, with the longest season being the Martian winter, which lasts for about nine months.\n",
            "\n",
            "4. **Methane and Water**: Scientists have detected methane and water ice on Mars. Methane is a key component in the search for life, as it could be a sign of microbial activity. Water ice is another vital resource, which could potentially be used as a source of drinking water or as a resource for future human settlements.\n",
            "\n",
            "5. **Current Exploration**: NASA's Curiosity rover has been exploring Mars since 2012 and has sent back a wealth of information about the planet's geology, atmosphere, and potential habitability. Future missions, such as the Mars 2020 Perseverance rover and the European Space Agency's ExoMars rover, aim to continue exploring Mars and search for signs of life.\n",
            "\n",
            "6. **Potential Habitability**: Mars is a fascinating target for potential human settlement in the future. With its relatively close proximity to Earth and its availability of resources, Mars could become an attractive destination for space agencies and private companies seeking to expand humanity's presence in the solar system.\n",
            "\n",
            "That's a brief overview of Mars, Alex. What aspect of the Red Planet would you like to know more about?\n",
            "\n",
            "Full history length: 3\n",
            "\n",
            "============================================================\n",
            "\n",
            "### RUN 2 (k=3) ###\n",
            "\n",
            "--- [Attempting Summarization at Run 2] ---\n",
            "Condition not met (Run 2 % 3 != 0). Skipping summarization.\n",
            "-------------------------------------------------\n",
            "\n",
            "USER: How long would it take to travel there from Earth?\n",
            "ASSISTANT: The travel time to Mars depends on a few factors, including the specific spacecraft design, the trajectory of the flight, and the starting and ending dates. However, based on current technology and available information, here are some estimated travel times to Mars:\n",
            "\n",
            "1. **With current chemical rockets**: The fastest spacecraft that has ever traveled to Mars is NASA's Mariner 4, which took around 6.1 months to cover a distance of approximately 225 million kilometers (140 million miles). However, this spacecraft was not designed for human exploration. More recent missions, like NASA's Curiosity rover and the Mars 2020 Perseverance rover, took around 6-7 months to reach Mars.\n",
            "\n",
            "2. **With current electric propulsion**: Electric propulsion systems, which use electricity to accelerate charged particles, can achieve higher speeds and longer mission durations. For example, NASA's Dawn spacecraft, which orbited asteroid Vesta and dwarf planet Ceres, achieved an average speed of about 20 kilometers per second (12 miles per second). Assuming a similar propulsion system for a Mars mission, the travel time could be reduced to around 3-4 months.\n",
            "\n",
            "3. **With hypothetical advanced propulsion systems**: Some concepts for advanced propulsion systems, such as fusion propulsion or light sails, could potentially reduce travel time to Mars to around 1-2 months. However, these ideas are still in the early stages of research and development, and significant technological breakthroughs would be required to make them practical.\n",
            "\n",
            "4. **Hibernation and suspended animation**: Some proposals have been made to suspend human consciousness or hibernate crew members during the long journey to Mars. This would potentially reduce the required food, water, and life support systems, as well as the psychological impact of extended spaceflight on the crew. However, this approach is still purely theoretical and requires significant research and development to become feasible.\n",
            "\n",
            "It's worth noting that NASA's Artemis program aims to return humans to the lunar surface by 2025 and establish a sustainable presence on the Moon. The next step would be to send humans to Mars in the 2030s. The exact travel time and mission duration would depend on various factors, including the specific mission design, the performance of the spacecraft, and any unforeseen challenges that might arise during the mission.\n",
            "\n",
            "Would you like to know more about the challenges of space travel or the possibilities of human exploration of Mars?\n",
            "\n",
            "Full history length: 5\n",
            "\n",
            "============================================================\n",
            "\n",
            "### RUN 3 (k=3) ###\n",
            "\n",
            "--- [Attempting Summarization at Run 3] ---\n",
            "Condition met (k=3). Summarizing history...\n",
            "Original History Length (non-system): 6 messages\n",
            "New Summary: Here's a concise summary of the conversation:\n",
            "\n",
            "**Key Topics:**\n",
            "\n",
            "1. Mars: Overview of the planet, its geological history, atmosphere, seasons, and potential habitability.\n",
            "2. Travel time to Mars: Estimated travel times using current technology, electric propulsion, and hypothetical advanced propulsion systems.\n",
            "3. Jupiter's Great Red Spot (GRS): Description of the anticyclonic storm, its unique color, massive scale, rotation period, long history, changes over time, and atmospheric dynamics.\n",
            "\n",
            "**Questions Asked:**\n",
            "\n",
            "1. How long would it take to travel to Mars from Earth?\n",
            "2. What about Jupiter's 'Great Red Spot'?\n",
            "\n",
            "**Main Answers Given:**\n",
            "\n",
            "1. Travel time to Mars: Estimated travel times range from 6-7 months with current chemical rockets to 1-2 months with hypothetical advanced propulsion systems.\n",
            "2. Jupiter's Great Red Spot: The GRS is a persistent anticyclonic storm on Jupiter, characterized by a unique color, massive scale, and slow rotation period. It has undergone significant changes over time and is thought to be driven by Jupiter's rotation and atmospheric circulation patterns.\n",
            "History has been replaced with summary. New history length: 2\n",
            "-------------------------------------------------\n",
            "\n",
            "USER: What about Jupiter's 'Great Red Spot'?\n",
            "ASSISTANT: Jupiter's Great Red Spot (GRS) is a fascinating feature, Alex. The Great Red Spot is a persistent anticyclonic storm on Jupiter, which has been raging for centuries. Here are some interesting facts about the GRS:\n",
            "\n",
            "1. **Anticyclonic storm**: The Great Red Spot is a high-pressure region in Jupiter's atmosphere, characterized by clockwise rotation. This is unlike most other storms on Jupiter, which rotate counterclockwise.\n",
            "\n",
            "2. **Unique color**: The GRS is a deep reddish-pink color, which is due to the presence of sulfur and phosphorus compounds in the cloud deck. These chemicals are thought to be present in the form of sulfuric acid and phosphoric acid droplets, which scatter light in a way that gives the spot its distinct color.\n",
            "\n",
            "3. **Massive scale**: The Great Red Spot is enormous, with a diameter of approximately 16,500 kilometers (10,250 miles). That's about two Earths side by side.\n",
            "\n",
            "4. **Rotation period**: The GRS rotates very slowly, taking about 300-400 days to complete one rotation. This is much slower than the rotation period of Jupiter itself, which is just under 10 hours.\n",
            "\n",
            "5. **Long history**: The Great Red Spot has been continuously observed since the late 19th century, and it's believed to have been present on Jupiter for at least 150 years. Some estimates suggest that it may have existed for as long as 350-400 years.\n",
            "\n",
            "6. **Changes over time**: Despite its long duration, the GRS has undergone significant changes over the years. It has shrunk and expanded, and its shape and color have varied. In the 19th century, it was much larger and more oval in shape. In recent years, it has shrunk significantly and become more elliptical.\n",
            "\n",
            "7. **Atmospheric dynamics**: The Great Red Spot is thought to be driven by Jupiter's rotation and its convection-driven atmospheric circulation patterns. The storm is sustained by a combination of factors, including the Coriolis force, the transfer of angular momentum, and the interaction with Jupiter's wind patterns.\n",
            "\n",
            "8. **Weather extremes**: The conditions within the Great Red Spot are extreme, with winds reaching up to 644 kilometers per hour (400 miles per hour). The storm is also thought to be incredibly deep, stretching from as little as 5 kilometers (3 miles) to as much as 300 kilometers (186 miles) below the cloud tops.\n",
            "\n",
            "The Great Red Spot is an incredible feature of Jupiter's atmosphere, offering insights into the planet's internal dynamics and atmospheric circulation patterns. Ongoing research and future observations by space missions like the Juno probe and the Europa Clipper mission will continue to reveal the secrets of this enigmatic storm.\n",
            "\n",
            "Would you like to know more about Jupiter's atmosphere or the ongoing research on the GRS?\n",
            "\n",
            "Full history length after summarization: 2\n",
            "\n",
            "============================================================\n",
            "\n",
            "### RUN 4 (k=3) ###\n",
            "Notice: The model will now use the summary as context.\n",
            "\n",
            "--- [Attempting Summarization at Run 4] ---\n",
            "Condition not met (Run 4 % 3 != 0). Skipping summarization.\n",
            "-------------------------------------------------\n",
            "\n",
            "USER: Remind me, what was the first planet I asked about?\n",
            "ASSISTANT: The first planet you asked about was Mars.\n",
            "\n",
            "Full history length: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if client:\n",
        "    # Create a new manager just for this demo\n",
        "    trunc_manager = ConversationManager(client, MODEL, summarize_every_k=999) # k=999 to prevent summarization\n",
        "\n",
        "    # Manually add a long history\n",
        "    trunc_manager.add_message(\"user\", \"This is message 1. What is 1+1?\")\n",
        "    trunc_manager.add_message(\"assistant\", \"1+1 equals 2.\")\n",
        "    trunc_manager.add_message(\"user\", \"This is message 3. What is 2+2?\")\n",
        "    trunc_manager.add_message(\"assistant\", \"2+2 equals 4. This is a slightly longer response to add characters.\")\n",
        "    trunc_manager.add_message(\"user\", \"This is message 5. What is 3+3?\")\n",
        "    trunc_manager.add_message(\"assistant\", \"3+3 equals 6. This is the final, most recent message in the history.\")\n",
        "\n",
        "    print(f\"Full history created with {len(trunc_manager.history)} messages (1 system + 6 conversation).\")\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # --- Demo 1: Limit by max_turns=1 ---\n",
        "    print(\"--- Demo: get_truncated_history(max_turns=1) ---\")\n",
        "    # Should return the system prompt + the last 2 messages (1 turn)\n",
        "    truncated_by_turn = trunc_manager.get_truncated_history(max_turns=1)\n",
        "\n",
        "    print(f\"Truncated history length: {len(truncated_by_turn)}\")\n",
        "    for msg in truncated_by_turn:\n",
        "        print(msg)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "    # --- Demo 2: Limit by max_chars=100 ---\n",
        "    print(\"--- Demo: get_truncated_history(max_chars=100) ---\")\n",
        "    # Should return system prompt + as many messages from the END as fit in 100 chars\n",
        "    truncated_by_char = trunc_manager.get_truncated_history(max_chars=100)\n",
        "\n",
        "    print(f\"Truncated history length: {len(truncated_by_char)}\")\n",
        "    total_chars = 0\n",
        "    for msg in truncated_by_char:\n",
        "        if msg['role'] != 'system':\n",
        "            total_chars += len(msg['content'])\n",
        "        print(msg)\n",
        "    print(f\"\\nTotal characters in conversation: {total_chars} (should be <= 100)\")\n",
        "\n",
        "else:\n",
        "    print(\"Client not initialized. Skipping Truncation Demonstration.\")"
      ],
      "metadata": {
        "id": "MKYUFJ6FCKR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbe421e-f31f-47d6-f183-77b44d73d4cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full history created with 7 messages (1 system + 6 conversation).\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Demo: get_truncated_history(max_turns=1) ---\n",
            "Truncated history length: 3\n",
            "{'role': 'system', 'content': 'You are a helpful assistant.'}\n",
            "{'role': 'user', 'content': 'This is message 5. What is 3+3?'}\n",
            "{'role': 'assistant', 'content': '3+3 equals 6. This is the final, most recent message in the history.'}\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Demo: get_truncated_history(max_chars=100) ---\n",
            "Truncated history length: 3\n",
            "{'role': 'system', 'content': 'You are a helpful assistant.'}\n",
            "{'role': 'user', 'content': 'This is message 5. What is 3+3?'}\n",
            "{'role': 'assistant', 'content': '3+3 equals 6. This is the final, most recent message in the history.'}\n",
            "\n",
            "Total characters in conversation: 99 (should be <= 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK - 2**"
      ],
      "metadata": {
        "id": "TFtm2GFer-lb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the JSON schema for our classifier tool\n",
        "# It tells the model *exactly* what structured data to return.\n",
        "CLASSIFIER_TOOL = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"classify_chat\",\n",
        "            \"description\": \"Classify the user's message into a category, sentiment, and extract key entities.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"category\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The main topic of the user's message.\",\n",
        "                        \"enum\": [\"support_request\", \"sales_inquiry\", \"general_query\", \"feedback\", \"other\"]\n",
        "                    },\n",
        "                    \"sentiment\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The emotional tone of the message.\",\n",
        "                        \"enum\": [\"positive\", \"neutral\", \"negative\"]\n",
        "                    },\n",
        "                    \"entities\": {\n",
        "                        \"type\": \"object\",\n",
        "                        \"description\": \"Any extracted entities like product names, emails, or order IDs.\",\n",
        "                        \"properties\": {\n",
        "                            \"product_name\": {\"type\": \"string\", \"description\": \"Any mentioned product names.\"},\n",
        "                            \"user_email\": {\"type\": \"string\", \"description\": \"The user's email address, if provided.\"},\n",
        "                            \"order_id\": {\"type\": \"string\", \"description\": \"Any mentioned order or ticket IDs.\"}\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"category\", \"sentiment\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "8UkXF_isCRUg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_message(user_message: str) -> Dict[str, Union[str, Dict]]:\n",
        "    \"\"\"\n",
        "    Classifies a single user message using Groq tool calling.\n",
        "    This version includes safety checks for the tool call response.\n",
        "    \"\"\"\n",
        "    if client is None:\n",
        "        return {\"error\": \"Groq client not initialized.\"}\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a classification assistant. Analyze the user's message and call the `classify_chat` tool with the appropriate category, sentiment, and extracted entities.\"},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            tools=CLASSIFIER_TOOL,\n",
        "            tool_choice={\"type\": \"function\", \"function\": {\"name\": \"classify_chat\"}},\n",
        "            temperature=0.1 # Lower temperature for more deterministic results\n",
        "        )\n",
        "\n",
        "        response_message = response.choices[0].message\n",
        "\n",
        "        # Check if the model decided to call a tool before trying to access it.\n",
        "        if response_message.tool_calls:\n",
        "            tool_call = response_message.tool_calls[0]\n",
        "            if tool_call.function.name == \"classify_chat\":\n",
        "                # The arguments are a JSON string, so we must parse them\n",
        "                arguments_json = tool_call.function.arguments\n",
        "                structured_data = json.loads(arguments_json)\n",
        "                return structured_data\n",
        "            else:\n",
        "                return {\"error\": \"Model called an unexpected tool.\"}\n",
        "        else:\n",
        "            # This block now handles cases where the model ignored our instruction\n",
        "            # and returned a plain text response instead.\n",
        "            error_content = response_message.content or \"No text content returned.\"\n",
        "            print(f\"DEBUG: Model failed to call a tool. It responded with: '{error_content}'\")\n",
        "            return {\n",
        "                \"error\": \"Model failed to generate a tool call.\",\n",
        "                \"model_response\": error_content\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return {\"error\": str(e)}"
      ],
      "metadata": {
        "id": "7qS-EFXvCRJA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if client:\n",
        "    test_messages = [\n",
        "        \"Hi, my X-1000 rocket boots are broken, can I get a replacement? My email is buzz@light.year and my order was #G-123.\",\n",
        "        \"I'm interested in buying the new Llama-3 model. How much is it?\",\n",
        "        \"What's the weather like in Tokyo today?\",\n",
        "        \"Your new UI is amazing! So much faster and easier to use. Great job!\",\n",
        "        \"I hate this new update. It's terrible. My order #T-456 is still missing. - angry_user@web.com\"\n",
        "    ]\n",
        "\n",
        "    for message in test_messages:\n",
        "        print(f\"--- INPUT ---\\n{message}\")\n",
        "        classification = classify_message(message)\n",
        "\n",
        "        # Pretty-print the JSON output\n",
        "        print(\"--- OUTPUT ---\")\n",
        "        print(json.dumps(classification, indent=2))\n",
        "        print(\"=\"*60)\n",
        "\n",
        "else:\n",
        "    print(\"Client not initialized. Skipping Task 2 Demonstration.\")"
      ],
      "metadata": {
        "id": "Ggg3rjiECSYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da43205-5664-46fa-88a4-642636846d70"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- INPUT ---\n",
            "Hi, my X-1000 rocket boots are broken, can I get a replacement? My email is buzz@light.year and my order was #G-123.\n",
            "--- OUTPUT ---\n",
            "{\n",
            "  \"category\": \"support_request\",\n",
            "  \"entities\": {\n",
            "    \"order_id\": \"G-123\",\n",
            "    \"user_email\": \"buzz@light.year\"\n",
            "  },\n",
            "  \"sentiment\": \"negative\"\n",
            "}\n",
            "============================================================\n",
            "--- INPUT ---\n",
            "I'm interested in buying the new Llama-3 model. How much is it?\n",
            "--- OUTPUT ---\n",
            "{\n",
            "  \"category\": \"sales_inquiry\",\n",
            "  \"entities\": {\n",
            "    \"product_name\": \"Llama-3 model\"\n",
            "  },\n",
            "  \"sentiment\": \"neutral\"\n",
            "}\n",
            "============================================================\n",
            "--- INPUT ---\n",
            "What's the weather like in Tokyo today?\n",
            "--- OUTPUT ---\n",
            "{\n",
            "  \"category\": \"general_query\",\n",
            "  \"entities\": {\n",
            "    \"product_name\": \"Tokyo\"\n",
            "  },\n",
            "  \"sentiment\": \"neutral\"\n",
            "}\n",
            "============================================================\n",
            "--- INPUT ---\n",
            "Your new UI is amazing! So much faster and easier to use. Great job!\n",
            "--- OUTPUT ---\n",
            "{\n",
            "  \"category\": \"general_query\",\n",
            "  \"entities\": {\n",
            "    \"product_name\": \"UI\",\n",
            "    \"user_email\": \"null\"\n",
            "  },\n",
            "  \"sentiment\": \"positive\"\n",
            "}\n",
            "============================================================\n",
            "--- INPUT ---\n",
            "I hate this new update. It's terrible. My order #T-456 is still missing. - angry_user@web.com\n",
            "--- OUTPUT ---\n",
            "{\n",
            "  \"category\": \"support_request\",\n",
            "  \"entities\": {\n",
            "    \"order_id\": \"T-456\",\n",
            "    \"user_email\": \"angry_user@web.com\"\n",
            "  },\n",
            "  \"sentiment\": \"negative\"\n",
            "}\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKgIuI0XCSTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDsbcNPvCSQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTxcdF-KCSOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}